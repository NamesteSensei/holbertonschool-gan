# Advanced GAN â€“ CelebA Image Generation

## ğŸ¯ Overview
This project builds upon the **DCGAN MNIST** project and explores a more complex challenge: generating realistic human face images using the **CelebA dataset**.  
The model was implemented using **PyTorch** and tracked via **Weights & Biases (W&B)** for full experiment visualization and metric tracking.

## ğŸ“Š Weights & Biases Project
- **W&B Project:** [Advanced GAN â€“ CelebA](https://wandb.ai/namestesensei-self/advanced-gan-celeba)
- Tracked metrics: Generator and Discriminator loss, generated samples, and hyperparameter logs.

## ğŸ§  Dataset
- **Dataset:** [CelebA â€“ Large-scale CelebFaces Attributes](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
- **Images used:** 11,611 aligned and cropped RGB images  
- **Resolution:** 64Ã—64  
- **Preprocessing:** Resizing, normalization to [-1, 1], and random horizontal flipping

## âš™ï¸ Model Architecture
The architecture is a **Deep Convolutional GAN (DCGAN)** with:
- **Generator:** Transposed convolutions, ReLU activations, BatchNorm layers  
- **Discriminator:** Strided convolutions, LeakyReLU activations, and Sigmoid output  
- **Loss:** Binary Cross-Entropy  
- **Optimizer:** Adam (lr = 0.0002, betas = (0.5, 0.999))

## ğŸ§ª Experiments
### 1ï¸âƒ£ Baseline GAN
Trained the standard DCGAN configuration for 3 epochs.  
The model began generating coherent facial structures and basic color patterns.

### 2ï¸âƒ£ Architecture Variation
Increased generator feature maps and adjusted kernel sizes.  
Improved image sharpness and reduced discriminator overfitting.

### 3ï¸âƒ£ Hyperparameter Tuning
Tuned batch size and learning rate; found **lr = 0.0002** and **batch_size = 64** achieved best stability.

## ğŸ’¾ Directory Structure
advanced_gan/
â”œâ”€â”€ data/ # CelebA dataset (subset or preprocessed images)
â”œâ”€â”€ models/ # Trained weights (.pth files)
â”œâ”€â”€ logs/ # Generated samples and W&B logs
â”œâ”€â”€ configs/ # Hyperparameter configuration files
â”œâ”€â”€ experiments/ # Notebooks for experiment tracking
â”œâ”€â”€ utils/ # Helper functions and training scripts
â””â”€â”€ README.md # Project overview and documentation


## ğŸ“ˆ Results
- Discriminator loss stabilized around **0.28**
- Generator loss converged near **3.26**
- Generated samples improved across epochs, showing sharper and more realistic features.

## ğŸ§© Key Takeaways
- Applying **lessons from MNIST DCGAN** (batch normalization, optimizer tuning) improved stability.
- Using **W&B** streamlined tracking and comparison between experiments.
- CelebA, being more complex, benefitted greatly from deeper architecture and regularization.

## ğŸ¥ Video Presentation
Loom/OBS link: *(Insert your presentation video link here)*

---

### ğŸš€ How to Run
```bash
python3 train_advanced_gan.py

